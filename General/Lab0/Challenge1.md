<!--- 

#Note to myself;
Make sure that you..
- show that you are thinking creatively
- apply principles of need finding
- satisfy the 5 tasks highlighted (see tldr) in this challenge
- note the phrase in the question, "without trying to offer a solution..."

-->

# Write-Up | CHALLENGE 1: Design Thinking
## Prepared by: Muhammad Fadli Alim Arsani
## Date: 09/23/2021

#### Answer:  
Consider the companies competing in the Autonomous Vehicles Industry, for instance Tesla, Comma AI, Waymo, and etc. They all have one goal, one task to execute: make cars driverless.  

The question I have in mind is, are they doing it right? *Disclaimer: This is strictly a personal opinion from a non-expert in the field, just an Electrical Engineering student, passionate in the idea of self-driving car.*  
 
The idea with most of these companies is by confining the cars to a set of rule. For example, if you see a car coming from the right, put a box on it, wait until it's safe to cross; if you see a red light, stop; if you see green, that's when it's good to go; and etc.

Are these really how __real humans__ drive? Do all drivers give or make way to other cars at times when they actually should? Do they not speed their way through the red lights sometimes? I wonder if by confining the algorithms that make self-driving cars to a set of fixed rules which assume humans are perfect drivers, will actually guarantee autonomous vehicles be a safer choice than a real human driver.

A YouTuber by the name, "JJRicks Studios" actively post videos of himself letting his self-driving car Waymo drives itself without ever touching the steering wheel. There are a lot of timestamps in most of his videos where the car was extremely unreliable and possesses danger to people using the road. Here are some of them:  

[Waymo Cuts Across Three Lanes](https://www.youtube.com/watch?v=nbS4uGXQtn0&t=576s)  

[Waymo Self Driving Taxi Goes Rogue: Blocks Traffic, Evades Capture](https://www.youtube.com/watch?v=zdKCQKBvH-A)  

[Tesla Autopilot Fails and Disengagements - Compilation](https://www.youtube.com/watch?v=yi5sVTewmXc)  

[Waymo Vehicle Gets Stuck, Then Abandons Me (Also I found a UI bug!)](https://www.youtube.com/watch?v=SH7cQojJlT8)  

[TESLA Car Crash Compilation 2021 - Autopilot Fails & Collision (2021 NEW)](https://www.youtube.com/watch?v=GxhNAN80c5s)

However, despite these fails (something that is a part of achieving a successful product), there are some cool clips where driverless vehicles actually worked and saved road users:  

[Top 10 Tesla Autopilot Saves](https://www.youtube.com/watch?v=bUhFfunT2ds)  

[UNBELIEVABLE Tesla Autopilot Saves Compilation (NEW 2021) | Tesla Autopilot predicts CRASH](https://www.youtube.com/watch?v=pHM6xY2Ys4U)  

Well, what if instead of somewhat "hard coding" a self-driving solution, maybe we need to train the car to drive like a real human, instead of teaching it to be a perfect driver because that is just not a reality. Consider what [comma ai](https://comma.ai/) is doing: they gather millions and millions of real driving data from all over the world through drivers who bought and use their product, then train this data and use it to model a "real" driverless vehicle behaviour. Overtime, it will only get better and better. Not only they cut off enormous figures from the cost of building an autonomous vehicle ([Waymo's LIDAR costs](https://www.forbes.com/sites/johanmoreno/2021/01/22/waymo-ceo-says-tesla-is-not-a-competitor-gives-estimated-cost-of-autonomous-vehicles/) about 75k USD, that's crazy), but they are also making it available for as many car models as they can (currently supporting a staggering 100+ vehicles). Plus, the data that they collected can itself be a business modal.

So this leaves me with one ultimate question, should we just train cars to drive themselves like perfect human beings or we let them make and adapt errors like real humans does? In the latter case, wouldn't that disregard the initial goal of making autonomous vehicles: safer road, i.e: no human error.  

This is something that we need to pause and ponder. It's either we make everyone own a self-driving car or we make the self-driving car so good that it can avoid any possible and unpredicatable mistakes regular drivers would execute on the road. :man_shrugging:
<br/> <!--- this is a line break -->
#### In summary:

- [x] The user of interest: __Autonomous Vehicles Companies.__
- [x] The task they perform: __Making cars driverless.__
- [x] The questions I asked:
- How should we make self-driving cars reliable?
- Are we doing it the right way?
- [x] Resources:
- YouTube
- Forbes
- comma AI
- [x] Problem Statement:
> "Should we just train cars to drive themselves like perfect human beings or we let them make and adapt errors like real humans does?" 	

